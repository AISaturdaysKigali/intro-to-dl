{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AISaturdaysKigali/intro-to-dl/blob/master/tutorials/W8_TimeSeriesAndNLP/W8_Tutorial1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Modeling sequencies and encoding text\n",
    "\n",
    "**Week 8: Modern RNNs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "----\n",
    "# Tutorial objectives\n",
    "\n",
    "Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "\n",
    "# @markdown There may be `Errors`/`Warnings` reported during the installation. However, they are to be ignored.\n",
    "!pip install torchtext==0.4.0 --quiet\n",
    "!pip install --upgrade gensim --quiet\n",
    "!pip install unidecode --quiet\n",
    "!pip install hmmlearn --quiet\n",
    "!pip install fasttext --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install python-Levenshtein --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/AISaturdaysKigali/content-creation/master/ai6kigali.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title  Load Dataset from `nltk`\n",
    "# no critical warnings, so we supress it\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "import requests\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "  #Tokenize the sentence\n",
    "  #from nltk.tokenize library use word_tokenize\n",
    "  token = word_tokenize(sentences)\n",
    "\n",
    "  return token\n",
    "\n",
    "\n",
    "def plot_train_val(x, train, val, train_label, val_label, title, y_label,\n",
    "                   color):\n",
    "  plt.plot(x, train, label=train_label, color=color)\n",
    "  plt.plot(x, val, label=val_label, color=color, linestyle='--')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel(y_label)\n",
    "  plt.title(title)\n",
    "\n",
    "\n",
    "def load_dataset(emb_vectors, sentence_length=50, seed=522):\n",
    "  TEXT = data.Field(sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    include_lengths=True,\n",
    "                    batch_first=True,\n",
    "                    fix_length=sentence_length)\n",
    "  LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "  TEXT.build_vocab(train_data, vectors=emb_vectors)\n",
    "  LABEL.build_vocab(train_data)\n",
    "\n",
    "  train_data, valid_data = train_data.split(split_ratio=0.7,\n",
    "                                            random_state=random.seed(seed))\n",
    "  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data,\n",
    "                                                                  valid_data,\n",
    "                                                                  test_data),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  sort_key=lambda x: len(x.text),\n",
    "                                                                  repeat=False,\n",
    "                                                                  shuffle=True)\n",
    "  vocab_size = len(TEXT.vocab)\n",
    "\n",
    "  print(f'Data are loaded. sentence length: {sentence_length} '\n",
    "        f'seed: {seed}')\n",
    "\n",
    "  return TEXT, vocab_size, train_iter, valid_iter, test_iter\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "  URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "  session = requests.Session()\n",
    "\n",
    "  response = session.get(URL, params={ 'id': id }, stream=True)\n",
    "  token = get_confirm_token(response)\n",
    "\n",
    "  if token:\n",
    "    params = { 'id': id, 'confirm': token }\n",
    "    response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "  save_response_content(response, destination)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "  for key, value in response.cookies.items():\n",
    "    if key.startswith('download_warning'):\n",
    "      return value\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "  CHUNK_SIZE = 32768\n",
    "\n",
    "  with open(destination, \"wb\") as f:\n",
    "    for chunk in response.iter_content(CHUNK_SIZE):\n",
    "      if chunk: # filter out keep-alive new chunks\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "DEVICE = set_device()\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Word Embeddings\n",
    "\n",
    "*Time estimate: ~60mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Words or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding. \n",
    "\n",
    "Word2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:\n",
    "\n",
    "... \"use the kitchen knife to chop the vegetables\"…\n",
    "\n",
    "**C1   C2   C3   T   C4   C5   C6   C7**\n",
    "\n",
    "Here, the target word is knife, and the context words are the ones in its immediate (6-word) window. \n",
    "The first word2vec method we’ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another. \n",
    "\n",
    "This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away. \n",
    "\n",
    "Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Creating Word Embeddings\n",
    "\n",
    "We will create embeddings for a subset of categories in [Brown corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html).  In order to achieve this task we will use [gensim](https://radimrehurek.com/gensim/) library to create word2vec embeddings. Gensim’s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.\n",
    "Calling `Word2Vec(sentences, iter=1)` will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. \n",
    "`Word2vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n",
    "\n",
    "`model = Word2Vec(sentences, min_count=10)  # default value is 5`\n",
    "\n",
    "\n",
    "A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
    "\n",
    "Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:\n",
    "\n",
    "`model = Word2Vec(sentences, size=200)  # default value is 100`\n",
    "\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
    "\n",
    "The last of the major parameters (full list [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:\n",
    "\n",
    "`model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion',\n",
    "            'reviews', 'romance', 'science_fiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def create_word2vec_model(category='news', size=50, sg=1, min_count=5):\n",
    "  try:\n",
    "    sentences = brown.sents(categories=category)\n",
    "    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n",
    "\n",
    "  except (AttributeError, TypeError):\n",
    "      raise AssertionError('Input variable \"category\" should be a string or list,'\n",
    "      '\"size\", \"sg\", \"min_count\" should be integers')\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_dictionary(model):\n",
    "  words = list(model.wv.key_to_index)\n",
    "  return words\n",
    "\n",
    "def get_embedding(word, model):\n",
    "  if word in model.wv.key_to_index:\n",
    "    return model.wv[word]\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "w2vmodel = create_word2vec_model(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(model_dictionary(w2vmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(get_embedding('weather', w2vmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Visualizing Word Embedding\n",
    "\n",
    "We can now obtain the word embeddings for any word in the dictionary using word2vec. Let's visualize these embeddings to get an inuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use `tSNE` (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use `tSNE` from [`scikit-learn`] module(https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (if you are not familiar with this method, think about `PCA`) to project our high dimensional embeddings in the 2D space.\n",
    "\n",
    "\n",
    "For each word in `keys`, we pick the top 10 similar words (using cosine similarity) and plot them.  \n",
    "\n",
    " What should be the arrangement of similar words?\n",
    " What should be arrangement of the key clusters with respect to each other?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "keys = ['voters', 'magic', 'love', 'God', 'evidence', 'administration', 'governments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def get_cluster_embeddings(keys):\n",
    "  embedding_clusters = []\n",
    "  word_clusters = []\n",
    "\n",
    "  # find closest words and add them to cluster\n",
    "  for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    if not word in w2vmodel.wv.key_to_index:\n",
    "      print('The word ', word, 'is not in the dictionary')\n",
    "      continue\n",
    "\n",
    "    for similar_word, _ in w2vmodel.wv.most_similar(word, topn=10):\n",
    "      words.append(similar_word)\n",
    "      embeddings.append(w2vmodel.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "  # get embeddings for the words in clusers\n",
    "  embedding_clusters = np.array(embedding_clusters)\n",
    "  n, m, k = embedding_clusters.shape\n",
    "  tsne_model_en_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "  return embeddings_en_2d, word_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters,\n",
    "                            word_clusters, a, filename=None):\n",
    "  plt.figure(figsize=(16, 9))\n",
    "  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "    x = embeddings[:, 0]\n",
    "    y = embeddings[:, 1]\n",
    "    plt.scatter(x, y, color=color, alpha=a, label=label)\n",
    "    for i, word in enumerate(words):\n",
    "      plt.annotate(word,\n",
    "                   alpha=0.5,\n",
    "                   xy=(x[i], y[i]),\n",
    "                   xytext=(5, 2),\n",
    "                   textcoords='offset points',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   size=10)\n",
    "  plt.legend(loc=\"lower left\")\n",
    "  plt.title(title)\n",
    "  plt.grid(True)\n",
    "  if filename:\n",
    "    plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "embeddings_en_2d, word_clusters = get_cluster_embeddings(keys)\n",
    "tsne_plot_similar_words('Similar words from Brown Corpus', keys, embeddings_en_2d, word_clusters, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Exploring meaning with word embeddings\n",
    "\n",
    "While word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# # @title Download FastText English Embeddings of dimension 100\n",
    "# import os, io, zipfile\n",
    "# from urllib.request import urlopen\n",
    "\n",
    "# zipurl = 'https://osf.io/w9sr7/download'\n",
    "# print(f\"Downloading and unzipping the file... Please wait.\")\n",
    "# with urlopen(zipurl) as zipresp:\n",
    "#   with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
    "#     zfile.extractall('.')\n",
    "# print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load 100 dimension FastText Vectors using FastText library\n",
    "ft_en_vectors = fasttext.load_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Length of the embedding is: {len(ft_en_vectors.get_word_vector('king'))}\")\n",
    "print(f\"Embedding for the word King is: {ft_en_vectors.get_word_vector('king')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now find the 10 most similar words to \"King\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ft_en_vectors.get_nearest_neighbors(\"king\", 10)  # Most similar by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "More on similarity between words. Let's check how similar different pairs of word are. Feel free to play around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def getSimilarity(word1, word2):\n",
    "  v1 = ft_en_vectors.get_word_vector(word1)\n",
    "  v2 = ft_en_vectors.get_word_vector(word2)\n",
    "  return cosine_similarity(v1, v2)\n",
    "\n",
    "print(\"Similarity between the words King and Queen: \", getSimilarity(\"king\", \"queen\"))\n",
    "print(\"Similarity between the words King and Knight: \", getSimilarity(\"king\", \"knight\"))\n",
    "print(\"Similarity between the words King and Rock: \", getSimilarity(\"king\", \"rock\"))\n",
    "print(\"Similarity between the words King and Twenty: \", getSimilarity(\"king\", \"twenty\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Homonym Words$^\\dagger$\n",
    "\n",
    "Find the similarity for homonym words with their different meanings. The first one has been implemented for you.\n",
    "\n",
    "\n",
    "$^\\dagger$: Two or more words having the same spelling or pronunciation but different meanings and origins are called *homonyms*. E.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#######################     Words with multiple meanings     ##########################\n",
    "print(\"Similarity between the words Cricket and Insect: \", getSimilarity(\"cricket\", \"insect\"))\n",
    "print(\"Similarity between the words Cricket and Sport: \", getSimilarity(\"cricket\", \"sport\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Analogies\n",
    "\n",
    "Embeddings can be used to find word analogies.\n",
    "Let's try it:\n",
    "1.   Man : Woman  ::  King : _____\n",
    "2.  Germany: Berlin :: France : ______\n",
    "3.  Leaf : Tree  ::  Petal : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Use get_analogies() funnction. The words have to be in the order Positive, negative,  Positve\n",
    "\n",
    "# Man : Woman  ::  King : _____\n",
    "# Positive=(woman, king), Negative=(man)\n",
    "print(ft_en_vectors.get_analogies(\"woman\", \"man\", \"king\",1))\n",
    "\n",
    "# Germany: Berlin :: France : ______\n",
    "# Positive=(berlin, frannce), Negative=(germany)\n",
    "print(ft_en_vectors.get_analogies(\"berlin\", \"germany\", \"france\",1))\n",
    "\n",
    "# Leaf : Tree  ::  Petal : _____\n",
    "# Positive=(tree, petal), Negative=(leaf)\n",
    "print(ft_en_vectors.get_analogies(\"tree\", \"leaf\", \"petal\",1))\n",
    "\n",
    "# Hammer : Nail  ::  Comb : _____\n",
    "# Positive=(nail, comb), Negative=(hammer)\n",
    "print(ft_en_vectors.get_analogies(\"nail\", \"hammer\", \"comb\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "But, does it always work?\n",
    "\n",
    "\n",
    "1.   Poverty : Wealth  :: Sickness : _____\n",
    "2.   train : board :: horse : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Poverty : Wealth  :: Sickness : _____\n",
    "print(ft_en_vectors.get_analogies(\"wealth\", \"poverty\", \"sickness\",1))\n",
    "\n",
    "# train : board :: horse : _____\n",
    "print(ft_en_vectors.get_analogies(\"board\", \"train\", \"horse\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Neural Net with word embeddings\n",
    "\n",
    "*Time estimate: ~16mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's use the pretrained FastText embeddings to train a neural network on the IMDB dataset. \n",
    "\n",
    "To recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We'll see detailed networks in the next tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 3.1: Simple Feed Forward Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This will load 300 dim FastText embeddings. It will take around 2-3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.\n",
    "The neural net will have one hidden layer of size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download embeddings and clear old variables to clean memory.\n",
    "# @markdown #### Execute this cell!\n",
    "if 'ft_en_vectors' in locals():\n",
    "  del ft_en_vectors\n",
    "if 'w2vmodel' in locals():\n",
    "  del w2vmodel\n",
    "\n",
    "embedding_fasttext = FastText('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Load the Dataset\n",
    "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(embedding_fasttext, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self, output_size, hidden_size, vocab_size, embedding_length,\n",
    "               word_embeddings):\n",
    "    super(NeuralNet, self).__init__()\n",
    "\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "    self.word_embeddings.weight = nn.Parameter(word_embeddings,\n",
    "                                               requires_grad=False)\n",
    "    self.fc1 = nn.Linear(embedding_length, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    input = self.word_embeddings(inputs)  # convert text to embeddings\n",
    "    ####################################################################\n",
    "    # Fill in missing code below (...)\n",
    "    raise NotImplementedError(\"Fill in the Neural Net\")\n",
    "    ####################################################################\n",
    "    # Average the word embeddings in a sentence\n",
    "    # Use torch.nn.functional.avg_pool2d to compute the averages\n",
    "    pooled = ...\n",
    "\n",
    "    # Pass the embeddings through the neural net\n",
    "    # A fully-connected layer\n",
    "    x = ...\n",
    "    # ReLU activation\n",
    "    x = ...\n",
    "    # Another fully-connected layer\n",
    "    x = ...\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment to check your code\n",
    "# nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)\n",
    "# print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "Solution not provided yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "NeuralNet(\n",
    "  (word_embeddings): Embedding(100, 300)\n",
    "  (fc1): Linear(in_features=300, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Training and Testing Functions\n",
    "\n",
    "# @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`\n",
    "# @markdown #### `test(model, device, test_iter)`\n",
    "\n",
    "def train(model, device, train_iter, valid_iter, epochs, learning_rate):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "    steps = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      # add micro for coding training loop\n",
    "      optimizer.zero_grad()\n",
    "      output = model(text)\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      steps += 1\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # get accuracy\n",
    "      _, predicted = torch.max(output, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "    train_loss.append(running_loss/len(train_iter))\n",
    "    train_acc.append(correct/total)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, '\n",
    "          f'Training Loss: {running_loss/len(train_iter):.4f}, '\n",
    "          f'Training Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "    # evaluate on validation data\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, batch in enumerate(valid_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    validation_loss.append(running_loss/len(valid_iter))\n",
    "    validation_acc.append(correct/total)\n",
    "\n",
    "    print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, '\n",
    "           f'Validation Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc\n",
    "\n",
    "\n",
    "def test(model, device, test_iter):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      outputs = model(text)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "learning_rate = 0.0003\n",
    "output_size = 2\n",
    "hidden_size = 128\n",
    "embedding_length = 300\n",
    "epochs = 15\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# Model set-up\n",
    "nn_model = NeuralNet(output_size,\n",
    "                     hidden_size,\n",
    "                     vocab_size,\n",
    "                     embedding_length,\n",
    "                     word_embeddings)\n",
    "nn_model.to(DEVICE)\n",
    "nn_start_time = time.time()\n",
    "set_seed(522)\n",
    "nn_train_loss, nn_train_acc, nn_validation_loss, nn_validation_acc = train(nn_model,\n",
    "                                                                           DEVICE,\n",
    "                                                                           train_iter,\n",
    "                                                                           valid_iter,\n",
    "                                                                           epochs,\n",
    "                                                                           learning_rate)\n",
    "print(f\"--- Time taken to train = {(time.time() - nn_start_time)} seconds ---\")\n",
    "test_accuracy = test(nn_model, DEVICE, test_iter)\n",
    "print(f'\\n\\nTest Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_acc, nn_validation_acc,\n",
    "               'train accuracy', 'val accuracy',\n",
    "               'Neural Net on IMDB text classification', 'accuracy',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(212)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_loss,\n",
    "               nn_validation_loss,\n",
    "               'train loss', 'val loss',\n",
    "               '',\n",
    "               'loss [a.u.]',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.\n",
    "\n",
    "The first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as ```hmmlearn``` that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well. \n",
    "\n",
    "The second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W2D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
